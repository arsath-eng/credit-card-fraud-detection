{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q4hkE2GCExx",
        "outputId": "61fe7e14-1af5-405c-922a-475acf5b5391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (284807, 31)\n",
            "Shape after cleaning: (283726, 31)\n",
            "Shape after transformation: (283726, 33)\n",
            "Shape after feature engineering: (283726, 35)\n",
            "Shape after balancing: (566506, 34)\n",
            "Training set shape: (453204, 34)\n",
            "Testing set shape: (113302, 34)\n",
            "\n",
            "Preprocessing completed. Data is now ready for modelling.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/creditcard.csv')\n",
        "\n",
        "# 1. Data Cleaning\n",
        "def clean_data(df):\n",
        "    # Handle missing values\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "    # Remove duplicates\n",
        "    df_imputed = df_imputed.drop_duplicates()\n",
        "\n",
        "    # Handle outliers (using IQR method for Amount)\n",
        "    Q1 = df_imputed['Amount'].quantile(0.25)\n",
        "    Q3 = df_imputed['Amount'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df_imputed['Amount'] = df_imputed['Amount'].clip(lower_bound, upper_bound)\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# 2. Data Transformation\n",
        "def transform_data(df):\n",
        "    # Normalize numerical features\n",
        "    scaler = StandardScaler()\n",
        "    numerical_features = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)]\n",
        "    df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "    # Create time-based features\n",
        "    df['Hour'] = df['Time'].apply(lambda x: (x / 3600) % 24)\n",
        "    df['Day'] = df['Time'].apply(lambda x: (x / 86400) % 7)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 3. Feature Engineering\n",
        "def engineer_features(df):\n",
        "    # Create a feature for transaction frequency per card\n",
        "    df['TransactionFreq'] = df.groupby('V1')['Time'].transform('count')\n",
        "\n",
        "    # Create a feature for average transaction amount per card\n",
        "    df['AvgAmount'] = df.groupby('V1')['Amount'].transform('mean')\n",
        "\n",
        "    return df\n",
        "\n",
        "# 4. Handle Imbalanced Data\n",
        "def balance_data(X, y):\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# 5. Data Splitting\n",
        "def split_data(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Main preprocessing pipeline\n",
        "def preprocess_data(df):\n",
        "    print(\"Original shape:\", df.shape)\n",
        "\n",
        "    # Clean data\n",
        "    df_cleaned = clean_data(df)\n",
        "    print(\"Shape after cleaning:\", df_cleaned.shape)\n",
        "\n",
        "    # Transform data\n",
        "    df_transformed = transform_data(df_cleaned)\n",
        "    print(\"Shape after transformation:\", df_transformed.shape)\n",
        "\n",
        "    # Engineer features\n",
        "    df_engineered = engineer_features(df_transformed)\n",
        "    print(\"Shape after feature engineering:\", df_engineered.shape)\n",
        "\n",
        "    # Prepare for modelling\n",
        "    X = df_engineered.drop(['Class'], axis=1)\n",
        "    y = df_engineered['Class']\n",
        "\n",
        "    # Balance data\n",
        "    X_resampled, y_resampled = balance_data(X, y)\n",
        "    print(\"Shape after balancing:\", X_resampled.shape)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = split_data(X_resampled, y_resampled)\n",
        "    print(\"Training set shape:\", X_train.shape)\n",
        "    print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Run the preprocessing pipeline\n",
        "X_train, X_test, y_train, y_test = preprocess_data(df)\n",
        "\n",
        "print(\"\\nPreprocessing completed. Data is now ready for modelling.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the training and testing data to CSV files\n",
        "X_train.to_csv('/content/drive/MyDrive/X_train.csv', index=False)\n",
        "X_test.to_csv('/content/drive/MyDrive/X_test.csv', index=False)\n",
        "y_train.to_csv('/content/drive/MyDrive/y_train.csv', index=False)\n",
        "y_test.to_csv('/content/drive/MyDrive/y_test.csv', index=False)\n",
        "\n",
        "print(\"\\nTraining and testing data saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SONquc67CwW5",
        "outputId": "3a1bceef-3aad-4f8c-862e-e43982debad0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and testing data saved to Google Drive.\n"
          ]
        }
      ]
    }
  ]
}